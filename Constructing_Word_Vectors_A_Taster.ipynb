{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfZOn9pYr0AtLKh0YYSdNr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jumblygrindrod/cooccurrences/blob/main/Constructing_Word_Vectors_A_Taster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing Word Vectors: A Taster"
      ],
      "metadata": {
        "id": "97oaWMCtYtI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This notebook will show you how you can produce a co-occurrence matrix using the Brown corpus. Once you have a co-occurrence matrix, you have a (relatively simple) vector space, with the rows corresponding to word vectors and the columns corresponding to the dimensions of the space.\n",
        "\n",
        "---\n",
        "\n",
        "### The advantage of hosting this here on Google Colab is that you can see each part of the code being run in real time without having to install python. Simply keep pressing shift + enter.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Each chunk of code comes with a little piece of text explaining what is happening at each stage.\n",
        "\n",
        "This notebook has largely been constructed using the python notebooks created by John Gamboa and Philip Blandfort. You can access those materials here: https://jcbgamboa.github.io/computational_linguistics/\n",
        "\n",
        "Their materials have been edited for the purposes of this notebook, in accordance with a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License  CC BY-NC-SA 4.0. This notebook is also licenced under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
      ],
      "metadata": {
        "id": "cUGR0mVrH8cO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we import the python packages that we'll need. One of the most important is nltk (Natural Language Toolkit), which is a package dedicated to pre-processing and processing textual data. But we'll also need some other packages (re, numpy, tabulate)."
      ],
      "metadata": {
        "id": "D9GWODST3eG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTqzPkUKnCOI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "k5Pm4aND02P6",
        "outputId": "12a2a599-8103-43c4-a0b6-80776a29790a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are getting the brown corpus from a url and introducing it as a string (i.e. one long object of text consisting of the entire corpus).\n",
        "\n",
        "(The nltk package actually provides the brown corpus and other corpora pre-tokenized, but for illustrative purposes we'll ignore that!)\n"
      ],
      "metadata": {
        "id": "b6cgi7aS3twF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(\"http://www.sls.hawaii.edu/bley-vroman/brown.txt\")\n",
        "text = response.text"
      ],
      "metadata": {
        "id": "isl1qr-MyBbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now tokenize the corpus (i.e. split it up into smaller linguistic chunks). First we will tokenize it into sentences (so we have a list of every sentence). Then we tokenize every sentence into a list of words. So we end up with a list of lists of words. (This can take a bit longer to complete)."
      ],
      "metadata": {
        "id": "AA96piNu31it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "sentences = [nltk.word_tokenize(s) for s in sentences]"
      ],
      "metadata": {
        "id": "IbT1WqLizuF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to remove non-words by requiring that each word only contains alphanumeric characters:"
      ],
      "metadata": {
        "id": "MnPq8fmp4DPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentences = []\n",
        "\n",
        "for s in sentences:\n",
        "    new_s = [w for w in s if re.fullmatch('[\\w]+', w)]\n",
        "    new_sentences.append(new_s)\n",
        "\n",
        "sentences = new_sentences"
      ],
      "metadata": {
        "id": "88oFtjV11WiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we are going to uncapitalize everything:"
      ],
      "metadata": {
        "id": "_mvdZDR54KSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentences = []\n",
        "\n",
        "for s in sentences:\n",
        "    # Creates a new list of tokens, uncapitalizing everything\n",
        "    new_s = [w.casefold() for w in s]\n",
        "    # Inserts the new list (i.e., `new_s`) in the new_sentences list\n",
        "    new_sentences.append(new_s)\n",
        "\n",
        "sentences = new_sentences"
      ],
      "metadata": {
        "id": "nWADuvRQ2EzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following just gives us a list of stopwords, which will be of use when constructing our matrix:"
      ],
      "metadata": {
        "id": "uBJ6_X2U-TzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "print(stopwords)"
      ],
      "metadata": {
        "id": "pftL4DEF1zhW",
        "outputId": "dfdbd430-58f9-40e8-e9fc-a1dc3af20723",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'doing', \"you're\", 'was', 'shouldn', 'll', 'ourselves', 'about', 'other', 'yourselves', 'all', 'that', \"shouldn't\", \"you've\", 'once', 'those', 'y', \"that'll\", 'here', 'o', 'after', 'he', 'is', 'yourself', 't', 'itself', 'under', 'just', 'been', 'are', \"hadn't\", 'who', 'out', 'then', 'doesn', \"doesn't\", \"you'll\", 'and', 'the', 'what', 'they', \"don't\", 'should', 'couldn', \"mightn't\", \"haven't\", 'but', 'below', 'ain', 'a', 'her', 'don', 'theirs', 'between', 'mustn', 'myself', 'while', 'too', \"you'd\", 'its', 'not', 're', 'haven', 'now', 'each', 'such', 'ma', \"didn't\", \"weren't\", 'you', 'because', \"wasn't\", 'by', 'am', 'does', 'further', 'our', \"it's\", 'were', \"shan't\", 'into', 'm', 'own', \"wouldn't\", 'being', 'through', 'from', 'mightn', 'will', 'why', 'down', 'aren', \"she's\", 'yours', 'me', 'up', 'had', 'him', 'an', 'his', 'hasn', 'weren', \"should've\", 'nor', 'until', 'of', 'these', 'my', \"mustn't\", 'shan', 'some', 'isn', 'during', 'if', 'has', 'more', 'very', 'in', 'both', 'himself', 'for', 'them', 'only', 'i', 'when', \"aren't\", 'against', 'd', 'than', 'with', 'off', \"hasn't\", 'having', 'hers', 'won', 'again', 'which', 'on', 'where', 'themselves', 'at', 'to', 'your', 'how', 'we', 'there', 'this', 'few', 'didn', 'needn', 'over', \"couldn't\", 'herself', 'have', 'whom', 'can', 've', \"isn't\", 'it', \"needn't\", 'no', 'ours', 'above', 'hadn', 'wouldn', 'she', 'wasn', 'did', 'do', 'or', 'as', 'same', 'most', 'their', 'be', 'any', 's', 'so', \"won't\", 'before'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are defining a function that will produce a co-occurrence matrix for us. This works by using the nltk function \"FreqDist\", which produces a dictionary (in Python's sense of the term) where the keys are words and the values are the number of times they appear. However, we want a more complex function because we only want the number of times that an expression appears within the window of the target expression we are interested in.\n",
        "\n",
        "There are a few parameters to play with here. You can change the window size. You can also change the vocab-size - where the vocab is the set of context words that occupy the columns of the cooccurrence matrix. Here the vocab is set to the words that are most common among all of the words of interest. You can also remove stopwords with this function.\n",
        "\n",
        "Note that the output of the function is two items: the vocab (i.e. the list of words that constitutes the columns) and the matrix itself."
      ],
      "metadata": {
        "id": "yPOgRHw24TcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_context_stuff(sentences, words, remove_stopwords=False, vocab_size=10, window_size=5):\n",
        "    co_occurrences = {word: nltk.FreqDist() for word in words}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in words:\n",
        "            if word in sentence:\n",
        "                word_pos = sentence.index(word)\n",
        "                co_occurrences[word].update([w.lower() for w in sentence[max(0,word_pos-window_size):min(word_pos+window_size,len(sentence)-1)]\n",
        "                                            if re.match('[\\w]+', w) and w!=word\n",
        "                                            and (not remove_stopwords or w.lower() not in stopwords)])\n",
        "\n",
        "    vocab = [c for c,count in nltk.FreqDist([w for word in co_occurrences\n",
        "                                             for w in co_occurrences[word]]).most_common(vocab_size)]\n",
        "\n",
        "    co_matrix = np.array([[co_occurrences[word][ctx] for ctx in vocab] for word in words])\n",
        "    return vocab, co_matrix"
      ],
      "metadata": {
        "id": "BlXlxi-02WqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function will display the matrix in an easy-to-read format, rather than just a list of arrays:"
      ],
      "metadata": {
        "id": "oEfRrv9m-0pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_co_matrix(mat, words, vocab, max_vocab_size=None):\n",
        "    if max_vocab_size:\n",
        "        print(tabulate([[word]+list(mat[i,:max_vocab_size]) for i,word in enumerate(words)],\n",
        "                       headers=['word \\ context']+vocab[:max_vocab_size]))\n",
        "    else:\n",
        "        print(tabulate([[word]+list(mat[i]) for i,word in enumerate(words)],\n",
        "                       headers=['word \\ context']+vocab))"
      ],
      "metadata": {
        "id": "O4nJxyLg3nvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need some words of interest that will form the rows in our matrix. Let's use a list of emotive words."
      ],
      "metadata": {
        "id": "bivqSQLwGM4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotive_words = nltk.word_tokenize(\"afraid angry calm cheerful cold crabby crazy cross excited frigid furious glad glum happy icy jolly jovial kind lively livid mad ornery rosy sad scared seething shy sunny tense tranquil upbeat wary weary worried\")"
      ],
      "metadata": {
        "id": "48WwzOhb2yWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's generate a matrix and vocab list with the function we defined above."
      ],
      "metadata": {
        "id": "0sTdG0F3Gd3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, matrix = compute_context_stuff(sentences, emotive_words, remove_stopwords = True, window_size = 10, vocab_size = 20)"
      ],
      "metadata": {
        "id": "6ttEJulD3U1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at the matrix using the show_co_matrix function:"
      ],
      "metadata": {
        "id": "silGh0uaHvZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_co_matrix(matrix, emotive_words, vocab)"
      ],
      "metadata": {
        "id": "r8ev2lu034qH",
        "outputId": "3a14b446-e7d7-439d-900b-fe9cd66f1c97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word \\ context      would    man    little    could    one    new    said    go    way    even    first    eyes    back    old    made    people    might    time    many    never\n",
            "----------------  -------  -----  --------  -------  -----  -----  ------  ----  -----  ------  -------  ------  ------  -----  ------  --------  -------  ------  ------  -------\n",
            "afraid                  4      1         0        0      6      1       1     0      2       2        1       0       1      0       0         5        2       2       1        1\n",
            "angry                   1      3         2        0      2      1       1     1      0       2        0       2       3      2       0         0        2       0       0        1\n",
            "calm                    0      0         0        1      0      0       2     0      0       0        1       1       0      1       0         1        0       1       0        0\n",
            "cheerful                0      0         0        0      0      0       0     0      1       0        0       1       0      0       1         0        0       0       0        0\n",
            "cold                    1      1         2        2      8      1       4     1      0       1        2       2       0      1       2         1        2       1       1        1\n",
            "crabby                  0      0         0        0      0      0       0     0      0       0        0       0       0      0       0         0        0       0       0        0\n",
            "crazy                   0      1         1        0      0      1       2     1      1       0        2       0       0      1       0         1        0       0       0        0\n",
            "cross                   2      0         0        2      0      1       0     0      0       1        1       0       0      0       1         0        0       2       2        1\n",
            "excited                 1      0         1        3      2      0       1     1      0       0        1       0       1      0       1         1        0       1       1        0\n",
            "frigid                  1      0         0        0      0      0       0     0      1       0        0       0       1      0       0         0        0       0       0        0\n",
            "furious                 0      0         0        0      0      0       0     0      0       0        0       0       1      0       0         0        0       0       0        0\n",
            "glad                    2      1         0        3      2      0       1     1      1       0        0       0       1      0       1         1        0       0       1        0\n",
            "glum                    0      0         1        0      0      0       0     0      0       0        0       0       0      0       0         0        0       0       0        0\n",
            "happy                   7      4         1        6      3      3       3     1      1       0        2       2       0      4       2         5        0       1       1        3\n",
            "icy                     0      0         0        1      1      0       0     1      0       0        0       0       0      0       0         0        0       0       0        0\n",
            "jolly                   1      0         0        0      0      0       0     0      0       0        0       0       0      1       0         0        0       0       0        0\n",
            "jovial                  0      0         0        0      0      0       0     0      0       1        0       0       0      0       0         0        0       0       0        0\n",
            "kind                   17     15         1       10     17      8       4     2      4       5        3       3       2      3       4        10        3       5       2        4\n",
            "lively                  0      1         0        0      0      1       0     1      0       1        0       1       0      2       1         0        1       0       0        0\n",
            "livid                   0      0         0        0      0      0       0     0      0       0        0       0       0      0       0         0        0       0       0        0\n",
            "mad                     2      1         0        2      0      0       1     0      1       1        0       1       0      1       0         0        1       0       0        0\n",
            "ornery                  0      0         0        0      0      0       0     1      0       0        0       0       0      0       0         0        0       0       0        0\n",
            "rosy                    0      0         0        1      0      0       0     0      0       2        1       0       0      0       0         0        0       0       0        0\n",
            "sad                     0      1         1        1      4      0       2     0      1       0        3       2       0      2       4         0        1       2       0        2\n",
            "scared                  2      1         0        0      0      1       2     0      1       0        0       0       1      0       0         0        0       0       0        0\n",
            "seething                0      0         0        0      0      0       0     0      0       0        0       0       0      0       0         0        0       0       0        0\n",
            "shy                     1      1         0        0      1      1       0     0      1       0        0       0       0      0       0         0        0       0       0        0\n",
            "sunny                   1      0         1        0      1      1       0     0      0       0        1       0       0      0       0         0        1       0       0        0\n",
            "tense                   0      0         0        0      0      0       0     1      0       1        0       0       0      0       0         0        0       1       0        0\n",
            "tranquil                1      0         0        0      0      0       0     0      0       1        0       0       0      0       0         0        0       0       0        0\n",
            "upbeat                  0      0         1        0      0      0       0     0      0       0        0       0       0      0       0         0        0       0       0        0\n",
            "wary                    0      0         1        0      0      0       0     0      0       0        0       1       1      0       1         0        0       0       0        0\n",
            "weary                   1      1         1        2      0      1       0     0      0       0        0       1       2      0       0         0        0       0       1        1\n",
            "worried                 1      0         1        1      1      0       0     1      0       0        0       0       0      0       0         1        1       0       1        1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above matrix shows co-occurence between the words of interest and the vocab words. For instance, it tells us that \"afraid\" co-occurs with \"would\" 4 times. But this can also serve as a vector space, with the rows serving as vectors for the words of interest.\n"
      ],
      "metadata": {
        "id": "EHQU0MFqTrRb"
      }
    }
  ]
}